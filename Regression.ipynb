{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Part a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Import and clean data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.clean_data import clean_data\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "df = pd.read_csv('data/diamond.csv')\n",
    "\n",
    "# performs interger encoding and log transforms carat and price.\n",
    "df = clean_data(df)\n",
    "\n",
    "\n",
    "df = df.drop([\"carat\", \"price\"], axis=1)\n",
    "\n",
    "df = (df - df.mean() ) / df.std(ddof=1)\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With log price\n",
    "Y = df['log_price'].values\n",
    "\n",
    "# with price\n",
    "#Y = Y_price\n",
    "\n",
    "feature_names = df.columns\n",
    "\n",
    "df = df.drop('log_price', axis=1)\n",
    "\n",
    "X = df.values\n",
    "\n",
    "\n",
    "N, M = X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Find the best regulzeration paramaetetr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "reg_tries = [0, 0.00001, 0.0001, 0.001, 0.01, 0.1, 10, 100, 1000, 5000, 10000]\n",
    "results = []\n",
    "\n",
    "for reg_value in reg_tries: \n",
    "    ridge_model = Ridge(alpha=reg_value)\n",
    "    \n",
    "    scores = cross_val_score(ridge_model, X_train, Y_train, cv=10, scoring=\"neg_root_mean_squared_error\")\n",
    "    \n",
    "    mean_rmse = -scores.mean()  # Convert negative RMSE back to positive\n",
    "    results.append((reg_value, mean_rmse))\n",
    "    print(f\"Regularization: {reg_value:<10} Cross-Validation RMSE: {mean_rmse:.8f}\")\n",
    "\n",
    "reg_values, rmse_values = zip(*results)\n",
    "\n",
    "# optimal reg model # with lowest RSME\n",
    "opt_reg = reg_values[rmse_values.index(min(rmse_values))]\n",
    "\n",
    "# train model with all features and optimal reg we found\n",
    "model = Ridge(alpha=opt_reg)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate RMSE for the test set\n",
    "test_rmse_all_features = np.sqrt(mean_squared_error(Y_test, y_pred))\n",
    "\n",
    "print(\"\\nFinal model :\")\n",
    "print(f\"Test RMSE: {test_rmse_all_features:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(reg_values, rmse_values, marker='o', linestyle='-', color='b')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Regularization Strength (alpha)')\n",
    "plt.ylabel('Cross-Validation RMSE')\n",
    "plt.title('Effect of Regularization on RMSE')\n",
    "plt.grid(True)\n",
    "plt.show()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Feature selection:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Backwards selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "# Backward selection: Start with all features and remove one at a time\n",
    "backward_selector = SequentialFeatureSelector(\n",
    "    Ridge(alpha=opt_reg),  # model with optimal reg.\n",
    "    n_features_to_select=\"auto\",\n",
    "    direction=\"backward\", \n",
    "    cv=10, \n",
    "    scoring=\"neg_root_mean_squared_error\"\n",
    ")\n",
    "backward_selector.fit(X_train, Y_train)\n",
    "\n",
    "# Get selected feature indices\n",
    "selected_backward_features = backward_selector.get_support(indices=True)\n",
    "print(f\"Selected Features (Backward): {selected_backward_features}\")\n",
    "\n",
    "\n",
    "X_train_backward = X_train[:, selected_backward_features]\n",
    "\n",
    "# backward model evaluate\n",
    "backward_score = cross_val_score(Ridge(alpha=opt_reg), X_train_backward, Y_train, cv=10, scoring=\"neg_root_mean_squared_error\")\n",
    "\n",
    "print(\"Backward score:\", -backward_score.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Forward Selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward selection: Start with no features and add one at a time\n",
    "forward_selector = SequentialFeatureSelector(\n",
    "    Ridge(alpha=opt_reg),  # model with optimal reg.\n",
    "    n_features_to_select=\"auto\",\n",
    "    direction=\"forward\", \n",
    "    cv=10, \n",
    "    scoring=\"neg_root_mean_squared_error\"\n",
    ")\n",
    "forward_selector.fit(X_train, Y_train)\n",
    "\n",
    "# Get selected feature indices\n",
    "selected_forward_features = forward_selector.get_support(indices=True)\n",
    "print(f\"Selected Features (Forward): {selected_forward_features}\")\n",
    "\n",
    "X_train_forward = X_train[:, selected_forward_features]\n",
    "\n",
    "# Forward model evaluation\n",
    "forward_score = cross_val_score(Ridge(alpha=opt_reg), X_train_forward, Y_train, cv=10, scoring=\"neg_root_mean_squared_error\")\n",
    "\n",
    "print(\"Forward score:\", -forward_score.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Final models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train forward model\n",
    "model_forward = Ridge(alpha=opt_reg)\n",
    "model_forward.fit(X_train_forward, Y_train)\n",
    "# Predict and calculate RMSE for forward model\n",
    "y_pred_forward = model_forward.predict(X_test[:, selected_forward_features])\n",
    "test_rmse_forward = np.sqrt(mean_squared_error(Y_test, y_pred_forward))\n",
    "\n",
    "# Train backward model\n",
    "model_backward = Ridge(alpha=opt_reg)\n",
    "model_backward.fit(X_train_backward, Y_train)\n",
    "# Predict and calculate RMSE for backward model\n",
    "y_pred_backward = model_backward.predict(X_test[:, selected_backward_features])\n",
    "test_rmse_backward = np.sqrt(mean_squared_error(Y_test, y_pred_backward))\n",
    "\n",
    "\n",
    "# Calculate R^2 values\n",
    "r2_all_features = model.score(X_test, Y_test)\n",
    "r2_forward = model_forward.score(X_test[:, selected_forward_features], Y_test)\n",
    "r2_backward = model_backward.score(X_test[:, selected_backward_features], Y_test)\n",
    "\n",
    "# Print R^2 values\n",
    "print(f\"R^2 All features          : {r2_all_features:.8f}\")\n",
    "print(f\"R^2 Forward model         : {r2_forward:.8f}\")\n",
    "print(f\"R^2 Backward model        : {r2_backward:.8f}\")\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nTest RMSE All features          : {test_rmse_all_features:.8f}\")\n",
    "print(f\"Test RMSE Forward model         : {test_rmse_forward:.8f}\")\n",
    "print(f\"Test RMSE Backward model        : {test_rmse_backward:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nForward Model Feature Weights:\")\n",
    "# print feature weight for all features model\n",
    "weights = model.coef_\n",
    "for name, weight in zip(feature_names, weights):\n",
    "    print(f\"Feature: {name:<10} Weight: {weight:.8f}\")\n",
    "\n",
    "# Print feature weights for forward model\n",
    "print(\"\\nForward Model Feature Weights:\")\n",
    "forward_weights = model_forward.coef_\n",
    "for name, weight in zip(feature_names[selected_forward_features], forward_weights):\n",
    "    print(f\"Feature: {name:<10} Weight: {weight:.8f}\")\n",
    "\n",
    "# Print feature weights for backward model\n",
    "print(\"\\nBackward Model Feature Weights:\")\n",
    "backward_weights = model_backward.coef_\n",
    "for name, weight in zip(feature_names[selected_backward_features], backward_weights):\n",
    "    print(f\"Feature: {name:<10} Weight: {weight:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "# PART b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "# Find optima ann model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_ann_model(x_train, y_train, epochs=10, learning_rate=0.01):\n",
    "    layer_sizes = [8, 16, 32, 64]\n",
    "    results = []\n",
    "    # Define the model\n",
    "\n",
    "    # trains a one model for each layer size\n",
    "    for size in layer_sizes:\n",
    "        print(\"training with size: \", size)\n",
    "        \n",
    "        kf = KFold(n_splits=10, shuffle=True, random_state=42)  # K-Fold setup\n",
    "        fold_rmse = []  # Store RMSE scores for each fold\n",
    "        \n",
    "        for train_idx, val_idx in kf.split(x_train):\n",
    "            x_train_fold, x_val_fold = x_train[train_idx], x_train[val_idx]\n",
    "            y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "            model = keras.Sequential([\n",
    "                keras.layers.Dense(size, activation='tanh', input_shape=(x_train.shape[1],)),\n",
    "                keras.layers.Dense(1) \n",
    "            ])\n",
    "        \n",
    "            model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "                    loss='mse')  # Mean Squared Error for regression\n",
    "            \n",
    "            model.fit(x_train_fold, y_train_fold, epochs=epochs, verbose=0)\n",
    "            \n",
    "            y_pred = model.predict(x_val_fold)\n",
    "            rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred))  # Compute RMSE\n",
    "            fold_rmse.append(rmse)        \n",
    "            \n",
    "            \n",
    "        mean_rmse = np.mean(fold_rmse)  # Average RMSE over all folds\n",
    "        print(mean_rmse)\n",
    "        fold_rmse.append((size, mean_rmse))        \n",
    "    \n",
    "    best_size, best_rmse = min(results, key=lambda x: x[1])\n",
    "    print(f\"\\nBest layer size: {best_size} with RMSE: {best_rmse:.8f}\")\n",
    "    \n",
    "    \n",
    "    # Train final model with best layer size\n",
    "    final_model = keras.Sequential([\n",
    "        keras.layers.Dense(best_size, activation='tanh', input_shape=(x_train.shape[1],) ),\n",
    "        keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    final_model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate), loss='mse')\n",
    "    final_model.fit(x_train, y_train, epochs=epochs, verbose=1)  # Train on full dataset\n",
    "\n",
    "    return best_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#layer = find_best_ann_model(X_train, Y_train, epochs=10, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_baseline(x_train, y_train):\n",
    "    # Baseline model: Predicts the mean of the training target values\n",
    "    mean_value = np.mean(y_train)\n",
    "    print(f\"Baseline model mean value: {mean_value:.8f}\")\n",
    "    return mean_value\n",
    "\n",
    "\n",
    "def train_ann(x_train, y_train, layers_count, epochs=10, learning_rate=0.01):\n",
    "    best_size, best_rmse = min(results, key=lambda x: x[1])\n",
    "    print(f\"\\nBest layer size: {best_size} with RMSE: {best_rmse:.8f}\")\n",
    "    \n",
    "    # Train final model with best layer size\n",
    "    final_model = keras.Sequential([\n",
    "        keras.layers.Dense(layers_count, activation='tanh', input_shape=(x_train.shape[1],)),\n",
    "        keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    final_model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate), loss='mse')\n",
    "    final_model.fit(x_train, y_train, epochs=epochs, verbose=1)  # Train on full dataset\n",
    "\n",
    "    return final_model\n",
    "    \n",
    "    \n",
    "def train_ridge(x_train, y_train, optimal_reg, feature_selector):\n",
    "    model = Ridge(alpha=optimal_reg)\n",
    "    model.fit(x_train, y_train)\n",
    "    return model\n",
    "    \n",
    "\n",
    "def evaluate_model(model, x_test_cv, y_test_cv):\n",
    "    y_pred_cv = model.predict(x_test_cv)\n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_cv, y_pred_cv))\n",
    "    # Calculate R^2\n",
    "    r2 = r2_score(y_test_cv, y_pred_cv)\n",
    "    return rmse, r2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# setup Cross validation K FOLD\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "int = 0\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    x_train_cv, x_test_cv = X_train[train_index], X_train[test_index]\n",
    "    y_train_cv, y_test_cv = Y_train[train_index], Y_train[test_index]\n",
    "    \n",
    "    # MODEL 1 ANN model\n",
    "    ann_model = train_ann(x_train_cv, y_train_cv)\n",
    "    ann_rmse, ann_r2 = evaluate_model(ann_model, x_test_cv, y_test_cv)\n",
    "    print(f\"RIDGE: RMSE: {ann_rmse:.6f} R2: {ann_r2:.6f}\")\n",
    "    # Model 2 Ridge model from part a\n",
    "    # Gets index for features from section a:\n",
    "    selected_features = forward_selector.get_support(indices=True)\n",
    "    ridge_model = train_ridge(x_train_cv, y_train_cv, opt_reg, selected_features)\n",
    "    \n",
    "    ridge_rmse, ridge_r2 = evaluate_model(ridge_model, x_test_cv, y_test_cv)\n",
    "    print(f\"RIDGE: RMSE: {ridge_rmse:.6f} R2: {ridge_r2:.6f}\")\n",
    "    \n",
    "    ## Model 3: Baseline\n",
    "    #train_baseline(x_train_cv, y_train_cv)\n",
    "    \n",
    "    int += 1\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
